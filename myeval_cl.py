import os

import sys
path = [ 'D:\Pranet_vo2']
sys.path.extend(path)
print(path)

import cv2
import numpy as np
from skimage import io, color
from skimage.metrics import structural_similarity as ssim
from scipy.ndimage import zoom
from skimage.measure import label, regionprops
import matplotlib.pyplot as plt
import os, argparse
from utils.dataloader_cl_v2 import get_loaders
from scipy.ndimage import gaussian_filter, distance_transform_edt
from skimage.transform import resize
import pandas as pd
from config_test import get_parser


# Function Definitions
"""
Evaluation utilities for saliency maps and segmentation masks.

This script provides a set of metric implementations (MAE, F-measure,
E-measure, S-measure, structure measure, IoU, Dice, etc.) and a
command-line evaluation loop that aggregates per-image results into
summary tables and Excel reports. It is intended to evaluate predicted
saliency/segmentation maps against binary ground-truth masks.

Notes:
- The main evaluation loop reads predicted result maps from a results
    directory generated by the model and compares them to ground-truth
    masks to compute thresholded metrics across many thresholds.
"""

def normalize(img):
    """Normalize an image array to the range [0, 1].

    Parameters:
        img (numpy.ndarray): Input image or map.

    Returns:
        numpy.ndarray: Normalized image in floating point [0,1].
    """
    return (img - np.min(img)) / (np.max(img) - np.min(img))

def precision_and_recall(pred, gt):
    """Compute precision and recall between a prediction and ground truth.

    Parameters:
        pred (array-like): Predicted binary map.
        gt (array-like): Ground-truth binary map.

    Returns:
        tuple: (precision, recall)
    """
    pred = pred.astype(bool)
    gt = gt.astype(bool)
    true_positive = np.sum(pred & gt)
    false_positive = np.sum(pred & ~gt)
    false_negative = np.sum(~pred & gt)
    
    precision = true_positive / (true_positive + false_positive + 1e-8)
    recall = true_positive / (true_positive + false_negative + 1e-8)
    
    return precision, recall

def iou(pred, gt):
    """Compute Intersection over Union (IoU) for two binary masks."""
    intersection = np.logical_and(pred, gt)
    union = np.logical_or(pred, gt)
    iou_score = np.sum(intersection) / np.sum(union)
    return iou_score

def dice_coefficient(pred, gt):
    """Compute Dice (F1) coefficient between two binary masks."""
    intersection = np.sum(pred * gt)
    return (2. * intersection) / (np.sum(pred) + np.sum(gt))



def emeasure(FM, GT):
    """Compute the Enhanced-alignment Measure (E-measure).

    This implementation follows the enhanced-alignment formulation used
    for saliency evaluation. It compares a binary/foreground map `FM`
    to the ground-truth mask `GT` and returns a scalar alignment score.

    Parameters:
        FM (numpy.ndarray): Foreground prediction (binary or probabilistic).
        GT (numpy.ndarray): Ground-truth binary mask.

    Returns:
        float: Enhanced-alignment score (E-measure).
    """
    FM = FM.astype(bool)
    GT = GT.astype(bool)
    
    dFM = FM.astype(float)
    dGT = GT.astype(float)
    
    def alignment_term(dFM, dGT):
        """Compute the alignment matrix"""
        mu_FM = np.mean(dFM)
        mu_GT = np.mean(dGT)
        
        align_FM = dFM - mu_FM
        align_GT = dGT - mu_GT
        
        align_matrix = 2 * (align_GT * align_FM) / (align_GT**2 + align_FM**2 + np.finfo(float).eps)
        return align_matrix
    
    def enhanced_alignment_term(align_matrix):
        """Compute the enhanced alignment matrix"""
        return ((align_matrix + 1)**2) / 4
    
    if np.sum(dGT) == 0:  # if GT is completely black
        enhanced_matrix = 1.0 - dFM
    elif np.sum(~GT) == 0:  # if GT is completely white
        enhanced_matrix = dFM
    else:
        align_matrix = alignment_term(dFM, dGT)
        enhanced_matrix = enhanced_alignment_term(align_matrix)
    
    w, h = GT.shape
    score = np.sum(enhanced_matrix) / (w * h - 1 + np.finfo(float).eps)
    return score

def cal_mae(smap, gt_img):
    """Calculate Mean Absolute Error (MAE) between saliency map and GT.

    MAE is computed as the average absolute difference between the
    predicted saliency values and the binary ground truth mask.

    Parameters:
        smap (numpy.ndarray): Predicted saliency map (float values).
        gt_img (numpy.ndarray): Ground-truth mask (binary or image).

    Returns:
        float: Mean absolute error.
    """
    if smap.shape[:2] != gt_img.shape[:2]:
        raise ValueError("Saliency map and ground truth image have different sizes!")

    if gt_img.dtype != np.bool_:
        gt_img = gt_img[:, :, 0] > 128

    smap = smap.astype(np.float64)
    fg_pixels = smap[gt_img]
    fg_err_sum = len(fg_pixels) - np.sum(fg_pixels)
    bg_err_sum = np.sum(smap[~gt_img])
    mae = (fg_err_sum + bg_err_sum) / gt_img.size
    return mae

def fmeasure_calc(sMap, gtMap, gtsize, threshold):
    """Compute thresholded segmentation metrics (precision, recall, etc.).

    Thresholds the saliency map `sMap` and computes confusion-matrix
    derived metrics against `gtMap` for a single threshold.

    Returns: (precision, recall, specificity, dice, fmeasure, iou)
    """
    if threshold > 1:
        threshold = 1

    Label3 = np.zeros(gtsize, dtype=bool)
    Label3[sMap >= threshold] = 1

    gtMap = gtMap.astype(bool)  # 确保gtMap是布尔类型

    NumRec = np.sum(Label3 == 1)  # FP + TP
    NumNoRec = np.sum(Label3 == 0)  # FN + TN
    LabelAnd = Label3 & gtMap
    NumAnd = np.sum(LabelAnd == 1)  # TP
    num_obj = np.sum(gtMap)  # TP + FN
    num_pred = np.sum(Label3)  # FP + TP

    FN = num_obj - NumAnd
    FP = NumRec - NumAnd
    TN = NumNoRec - FN

    if NumAnd == 0:
        PreFtem = 0
        RecallFtem = 0
        FmeasureF = 0
        Dice = 0
        SpecifTem = 0
        IoU = 0
    else:
        IoU = NumAnd / (FN + NumRec)  # TP / (FN + TP + FP)
        PreFtem = NumAnd / NumRec
        RecallFtem = NumAnd / num_obj
        SpecifTem = TN / (TN + FP)
        Dice = 2 * NumAnd / (num_obj + num_pred)
        FmeasureF = (2.0 * PreFtem * RecallFtem) / (PreFtem + RecallFtem)  # beta = 1.0

    return PreFtem, RecallFtem, SpecifTem, Dice, FmeasureF, IoU

def original_wfb(FG, GT):
    """Compute the original Weighted F-beta measure (WFb).

    WFb is a weighted variant of F-measure that accounts for spatial
    errors and is commonly used in saliency evaluation.

    Parameters:
        FG (numpy.ndarray): Predicted foreground map in [0,1].
        GT (numpy.ndarray): Ground-truth binary mask.

    Returns:
        float: Weighted F-beta score.
    """
    if FG.dtype != np.float64:
        raise ValueError("FG should be of type: double")
    if FG.max() > 1 or FG.min() < 0:
        raise ValueError("FG should be in the range of [0 1]")
    if not np.issubdtype(GT.dtype, np.bool_):
        raise ValueError("GT should be of type: logical")

    dGT = GT.astype(np.float64)

    E = np.abs(FG - dGT)

    # Ensure GT is a boolean array for the invert operation
    GT_bool = GT.astype(bool)
    Dst, IDXT = distance_transform_edt(~GT_bool, return_indices=True)
    
    K = gaussian_filter(np.ones((7, 7)), sigma=5)
    Et = E.copy()
    Et[~GT_bool] = Et[tuple(IDXT[:, ~GT_bool])]
    
    EA = gaussian_filter(Et, sigma=5)
    MIN_E_EA = E.copy()
    MIN_E_EA[GT_bool & (EA < E)] = EA[GT_bool & (EA < E)]
    
    B = np.ones(GT.shape)
    B[~GT_bool] = 2.0 - 1 * np.exp(np.log(1 - 0.5) / 5 * Dst[~GT_bool])
    Ew = MIN_E_EA * B

    TPw = np.sum(dGT) - np.sum(Ew[GT_bool])
    FPw = np.sum(Ew[~GT_bool])

    R = 1 - np.mean(Ew[GT_bool])
    P = TPw / (np.finfo(float).eps + TPw + FPw)

    Q = 2 * (R * P) / (np.finfo(float).eps + R + P)
    
    return Q

def s_object(prediction, GT):
    """Compute the object-aware similarity (part of S-measure).

    This function evaluates how well object-level statistics (foreground
    and background) of the prediction match the ground truth.
    """
    def object_similarity(prediction, GT):
        """
        计算前景或背景的对象相似性得分
        
        参数:
            prediction (numpy.ndarray): 前景或背景图
            GT (numpy.ndarray): 真值图
        
        返回:
            float: 对象相似性得分
        """
        if prediction.size == 0:
            return 0.0
        if not np.issubdtype(prediction.dtype, np.float64):
            prediction = prediction.astype(np.float64)
        if prediction.max() > 1 or prediction.min() < 0:
            raise ValueError("prediction should be in the range of [0, 1]")
        if not np.issubdtype(GT.dtype, np.bool_):
            raise ValueError("GT should be of type: logical")

        x = np.mean(prediction[GT])
        sigma_x = np.std(prediction[GT])
        score = 2.0 * x / (x**2 + 1.0 + sigma_x + np.finfo(float).eps)
        return score

    prediction_fg = np.copy(prediction)
    prediction_fg[~GT] = 0
    O_FG = object_similarity(prediction_fg, GT)

    prediction_bg = 1.0 - prediction
    prediction_bg[GT] = 0
    O_BG = object_similarity(prediction_bg, ~GT)

    u = np.mean(GT)
    Q = u * O_FG + (1 - u) * O_BG
    
    return Q

def s_region(prediction, GT):
    """Compute the region-aware similarity (part of S-measure).

    The image and ground truth are divided around the centroid into
    four regions and structural similarity is computed per-region and
    aggregated with area weights.
    """
    def centroid(GT):
        rows, cols = GT.shape
        if np.sum(GT) == 0:
            X = round(cols / 2)
            Y = round(rows / 2)
        else:
            total = np.sum(GT)
            i = np.arange(cols)
            j = np.arange(rows)
            
            X = round(np.sum(np.sum(GT, axis=0) * i) / total)
            Y = round(np.sum(np.sum(GT, axis=1) * j) / total)
        return X, Y

    def divide_gt(GT, X, Y):
        hei, wid = GT.shape
        area = wid * hei

        LT = GT[:Y, :X]
        RT = GT[:Y, X:]
        LB = GT[Y:, :X]
        RB = GT[Y:, X:]

        w1 = (X * Y) / area
        w2 = ((wid - X) * Y) / area
        w3 = (X * (hei - Y)) / area
        w4 = 1.0 - w1 - w2 - w3

        return LT, RT, LB, RB, w1, w2, w3, w4

    def divide_prediction(prediction, X, Y):
        LT = prediction[:Y, :X]
        RT = prediction[:Y, X:]
        LB = prediction[Y:, :X]
        RB = prediction[Y:, X:]
        return LT, RT, LB, RB

    def ssim(prediction, GT):
        dGT = GT.astype(np.float64)
        prediction = prediction.astype(np.float64)

        N = prediction.size

        x = np.mean(prediction)
        y = np.mean(dGT)

        sigma_x2 = np.var(prediction)
        sigma_y2 = np.var(dGT)

        sigma_xy = np.mean((prediction - x) * (dGT - y))

        alpha = 4 * x * y * sigma_xy
        beta = (x**2 + y**2) * (sigma_x2 + sigma_y2)

        if alpha != 0:
            Q = alpha / (beta + np.finfo(float).eps)
        elif alpha == 0 and beta == 0:
            Q = 1.0
        else:
            Q = 0

        return Q

    X, Y = centroid(GT)

    GT_1, GT_2, GT_3, GT_4, w1, w2, w3, w4 = divide_gt(GT, X, Y)
    prediction_1, prediction_2, prediction_3, prediction_4 = divide_prediction(prediction, X, Y)

    Q1 = ssim(prediction_1, GT_1)
    Q2 = ssim(prediction_2, GT_2)
    Q3 = ssim(prediction_3, GT_3)
    Q4 = ssim(prediction_4, GT_4)

    Q = w1 * Q1 + w2 * Q2 + w3 * Q3 + w4 * Q4

    return Q

def structure_measure(prediction, GT):
    """Compute the Structure-measure (S-Measure) combining object
    and region similarity for saliency evaluation.

    Returns a single scalar summarizing structure-level agreement
    between `prediction` and `GT`.
    """
    if prediction.dtype != np.float64:
        raise ValueError("The prediction should be double type...")
    if prediction.max() > 1 or prediction.min() < 0:
        raise ValueError("The prediction should be in the range of [0 1]...")
    if not np.issubdtype(GT.dtype, np.bool_):
        raise ValueError("GT should be logical type...")

    y = np.mean(GT)

    if y == 0:  # if the GT is completely black
        x = np.mean(prediction)
        Q = 1.0 - x  # only calculate the area of intersection
    elif y == 1:  # if the GT is completely white
        x = np.mean(prediction)
        Q = x  # only calculate the area of intersection
    else:
        alpha = 0.5
        matter_1 = s_object(prediction, GT)
        matter_2 = s_region(prediction, GT)
        Q = alpha * matter_1 + (1 - alpha) * matter_2
        if Q < 0:
            Q = 0
    return Q







#main------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
parser = get_parser()
opt = parser.parse_args()

approches = opt.approches
# ---- 1. ResultMap Path Setting ----
ResultMapPath = os.path.join(opt.weights_dir, f'results/')
Models = ['PraNet_Res2Net']  # ['UNet', 'PraNet',]

modelNum = len(Models)

str = './EvaluateResults_polyp_gen/' + str( approches) + '/'

ResDir =os.path.join(opt.weights_dir, str)
ResName='_result.txt'

Thresholds = np.arange(1, -1, -1/255)


# Get all weight files in the directory
weight_files = [os.path.join(opt.weights_dir, f) for f in os.listdir(opt.weights_dir) if f.endswith('.pth')]


image_root = r'D:\PolypGen2021_MultiCenterData_v3' # set your image root directory
gt_root = r'D:\PolypGen2021_MultiCenterData_v3' 
data_loaders = get_loaders(image_root, batchsize=8, trainsize=256,ratio_set=[1,0,0])
# Main Evaluation Loop
# for weight_file in weight_files:
for weight_index, weight_file in enumerate(weight_files):

    weight_filename = os.path.basename(weight_file)
    # Extract the base name of the weight file without extension
    model_name = os.path.splitext(os.path.basename(weight_file))[0]
    for x, loaders in enumerate(data_loaders):
        if x > weight_index:
            break  # Skip loaders beyond the current weight file's range
        
        ResPath = os.path.join(ResDir, model_name, f'{loaders["name"]}-mat/')
        os.makedirs(ResPath, exist_ok=True)
        resTxt = os.path.join(ResDir, model_name, f'{loaders["name"]}{ResName}')
        model = Models[0]
        
        with open(resTxt, 'w') as fileID:
            print(f"Processing weight: {weight_filename}, using dataloader {loaders['name']}")
            gtPath = os.path.join(ResultMapPath, approches, model_name, loaders['name'], 'masks')
            resMapPath = os.path.join(ResultMapPath, approches, model_name, loaders['name'], 'outputs')
            
            img_files = [f for f in os.listdir(resMapPath) if f.endswith('.png')]
            img_num = len(img_files)
            
            threshold_fmeasure = np.zeros((img_num, len(Thresholds)))
            threshold_emeasure = np.zeros((img_num, len(Thresholds)))
            threshold_iou = np.zeros((img_num, len(Thresholds)))
            threshold_precision = np.zeros((img_num, len(Thresholds)))
            threshold_recall = np.zeros((img_num, len(Thresholds)))
            threshold_sensitivity = np.zeros((img_num, len(Thresholds)))
            threshold_specificity = np.zeros((img_num, len(Thresholds)))
            threshold_dice = np.zeros((img_num, len(Thresholds)))
            
            smeasure = np.zeros(img_num)
            wfmeasure = np.zeros(img_num)
            data_mae = np.zeros(img_num)
            
            for i, img_file in enumerate(img_files):
                name = img_file
                print(f'Evaluating({weight_filename} {loaders["name"]} Dataset,{model} Model, {name} Image): {i + 1}/{img_num}')
                
                gt = io.imread(os.path.join(gtPath, name))
                if gt.ndim > 2:
                    gt = color.rgb2gray(gt)
                
                gt = gt > 128
                
                resmap = io.imread(os.path.join(resMapPath, name))
                
                if resmap.shape[:2] != gt.shape[:2]:
                    resmap = resize(resmap, gt.shape, anti_aliasing=True)
                    io.imsave(os.path.join(resMapPath, name), resmap)
                    print(f'Resizing has been operated!! The resmap size does not match with gt in the path: {os.path.join(resMapPath, name)}!!!')
                
                resmap = resmap.astype(np.float64)
                
                # normalize resmap to [0, 1]
                resmap = (resmap - resmap.min()) / (resmap.max() - resmap.min())
                
                smeasure[i] = structure_measure(resmap, gt)
                wfmeasure[i] = original_wfb(resmap, gt)
                data_mae[i] = cal_mae(resmap, gt)
                
                threshold_e = np.zeros(len(Thresholds))
                threshold_f = np.zeros(len(Thresholds))
                threshold_pr = np.zeros(len(Thresholds))
                threshold_rec = np.zeros(len(Thresholds))
                threshold_i = np.zeros(len(Thresholds))
                threshold_spe = np.zeros(len(Thresholds))
                threshold_dic = np.zeros(len(Thresholds))
                
                for t, threshold in enumerate(Thresholds):
                    threshold_pr[t], threshold_rec[t], threshold_spe[t], threshold_dic[t], threshold_f[t], threshold_i[t] = fmeasure_calc(resmap, gt, gt.shape, threshold)
                    
                    bi_resmap = resmap >= threshold
                    threshold_e[t] = emeasure(bi_resmap, gt)
                
                threshold_emeasure[i, :] = threshold_e
                threshold_fmeasure[i, :] = threshold_f
                threshold_sensitivity[i, :] = threshold_rec
                threshold_specificity[i, :] = threshold_spe
                threshold_dice[i, :] = threshold_dic
                threshold_iou[i, :] = threshold_i
  
            # Calculate metrics
            data_mae = np.mean(data_mae)
            sm = np.mean(smeasure)
            wf = np.nanmean(wfmeasure)
            
            column_e = np.mean(threshold_emeasure, axis=0)
            mean_em = np.mean(column_e)
            max_em = np.max(column_e)
            
            column_sen = np.mean(threshold_sensitivity, axis=0)
            mean_sen = np.mean(column_sen)
            max_sen = np.max(column_sen)
            
            column_spe = np.mean(threshold_specificity, axis=0)
            mean_spe = np.mean(column_spe)
            max_spe = np.max(column_spe)
            
            column_dic = np.mean(threshold_dice, axis=0)
            mean_dic = np.mean(column_dic)
            max_dic = np.max(column_dic)
            
            column_iou = np.mean(threshold_iou, axis=0)
            mean_iou = np.mean(column_iou)
            max_iou = np.max(column_iou)
                
            # Save results to Excel
            excel_path = os.path.join(ResPath, 'evaluation_results.xlsx')
            with pd.ExcelWriter(excel_path) as writer:
                for metric_name, data in {
                    "F-measure": threshold_fmeasure,
                    "E-measure": threshold_emeasure,
                    "IoU": threshold_iou,
                    "Precision": threshold_precision,
                    "Recall": threshold_recall,
                    "Sensitivity": threshold_sensitivity,
                    "Specificity": threshold_specificity,
                    "Dice": threshold_dice
                }.items():
                    df = pd.DataFrame(data, columns=Thresholds, index=img_files)
                    df.index.name = "Image Name"
                    df.columns.name = "Thresholds"
                    df.to_excel(writer, sheet_name=metric_name)
                
                summary = {
                    "MAE": data_mae,
                    "S-measure": sm,
                    "Weighted F-measure": wf,
                    "Mean E-measure": mean_em,
                    "Max E-measure": max_em,
                    "Mean Sensitivity": mean_sen,
                    "Max Sensitivity": max_sen,
                    "Mean Specificity": mean_spe,
                    "Max Specificity": max_spe,
                    "Mean Dice": mean_dic,
                    "Max Dice": max_dic,
                    "Mean IoU": mean_iou,

                    "Max IoU": max_iou
                }
                
                summary_df = pd.DataFrame(summary,index=["Value"])
                summary_df.to_excel(writer, sheet_name="Summary")

                for key, value in summary.items():
                    fileID.write(f"{key}: {value}\n")  

